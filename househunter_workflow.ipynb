{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HouseHunter Pro - Workflow\n",
    "\n",
    "**Workflow:**\n",
    "1. Detect scrape dates\n",
    "2. Load search results and compute URL diff\n",
    "3. Show new URLs to scrape\n",
    "4. Parse all listings\n",
    "5. Create final dataframe with: `is_sold`, `prices` (history), `days_live`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from scraping import (\n",
    "    process_listings_directory,\n",
    "    process_search_results_directory,\n",
    ")\n",
    "from scraping.utils import deduplicate_listings, fix_list_columns\n",
    "from scraping.derive_fields import derive_fields_for_dataset, serialize_derived_fields\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "SCRAPED_DIR = Path('data/scraped')\n",
    "DATA_DIR = Path('.')\n",
    "\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Detect Available Scrape Dates\n",
    "\n",
    "Auto-detect all scraped dates from the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 scrape dates:\n",
      "  2026_01_18 \n",
      "  2026_01_24 (current)\n"
     ]
    }
   ],
   "source": [
    "# Find all date directories (format: YYYY_MM_DD)\n",
    "import re\n",
    "\n",
    "date_dirs = []\n",
    "for d in SCRAPED_DIR.iterdir():\n",
    "    if d.is_dir() and re.match(r'\\d{4}_\\d{2}_\\d{2}', d.name):\n",
    "        date_dirs.append(d.name)\n",
    "\n",
    "# Sort chronologically\n",
    "date_dirs = sorted(date_dirs)\n",
    "\n",
    "print(f\"Found {len(date_dirs)} scrape dates:\")\n",
    "for i, d in enumerate(date_dirs):\n",
    "    label = \"(current)\" if i == len(date_dirs) - 1 else \"\"\n",
    "    print(f\"  {d} {label}\")\n",
    "\n",
    "# Current date is the most recent\n",
    "CURRENT_DATE = date_dirs[-1] if date_dirs else None\n",
    "ALL_DATES = date_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Load Search Results for All Dates\n",
    "\n",
    "Load search results from all scraped dates to find:\n",
    "- New URLs to scrape (appeared in current but not previous)\n",
    "- Sold URLs (disappeared between scrapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026_01_18: 157 unique listings\n",
      "2026_01_24: 157 unique listings\n",
      "\n",
      "Loaded search results for 2 dates\n"
     ]
    }
   ],
   "source": [
    "# Load search results for all dates\n",
    "search_results_by_date = {}\n",
    "\n",
    "for date_str in ALL_DATES:\n",
    "    search_dir = SCRAPED_DIR / date_str / 'search_results'\n",
    "    if search_dir.exists():\n",
    "        df = process_search_results_directory(search_dir, date_str)\n",
    "        if not df.empty:\n",
    "            # Deduplicate - keep first occurrence (lowest page number)\n",
    "            df = df.drop_duplicates(subset=['listing_id'], keep='first')\n",
    "            search_results_by_date[date_str] = df\n",
    "            print(f\"{date_str}: {len(df)} unique listings\")\n",
    "\n",
    "print(f\"\\nLoaded search results for {len(search_results_by_date)} dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: 2026_01_18 -> 2026_01_24\n",
      "  New: 13\n",
      "  Removed (sold): 13\n",
      "  Still active: 144\n"
     ]
    }
   ],
   "source": [
    "# Compute URL diffs and build price history\n",
    "sorted_dates = sorted(search_results_by_date.keys())\n",
    "\n",
    "# Track price history for each listing: listing_id -> [(date, price), ...]\n",
    "price_history = {}\n",
    "\n",
    "# Track when each listing was last seen (to detect sold)\n",
    "last_seen_date = {}\n",
    "\n",
    "# Build history from all dates\n",
    "for date_str in sorted_dates:\n",
    "    df = search_results_by_date[date_str]\n",
    "    for _, row in df.iterrows():\n",
    "        lid = row['listing_id']\n",
    "        price = row.get('price')\n",
    "        \n",
    "        if lid not in price_history:\n",
    "            price_history[lid] = []\n",
    "        price_history[lid].append((date_str, price))\n",
    "        last_seen_date[lid] = date_str\n",
    "\n",
    "# Compute new/removed for latest comparison\n",
    "if len(sorted_dates) >= 2:\n",
    "    prev_date = sorted_dates[-2]\n",
    "    curr_date = sorted_dates[-1]\n",
    "    \n",
    "    df_prev = search_results_by_date[prev_date]\n",
    "    df_curr = search_results_by_date[curr_date]\n",
    "    \n",
    "    prev_ids = set(df_prev['listing_id'].unique())\n",
    "    curr_ids = set(df_curr['listing_id'].unique())\n",
    "    \n",
    "    new_ids = curr_ids - prev_ids\n",
    "    removed_ids = prev_ids - curr_ids\n",
    "    \n",
    "    df_new_urls = df_curr[df_curr['listing_id'].isin(new_ids)].copy()\n",
    "    \n",
    "    print(f\"Comparison: {prev_date} -> {curr_date}\")\n",
    "    print(f\"  New: {len(new_ids)}\")\n",
    "    print(f\"  Removed (sold): {len(removed_ids)}\")\n",
    "    print(f\"  Still active: {len(curr_ids & prev_ids)}\")\n",
    "else:\n",
    "    df_new_urls = pd.DataFrame()\n",
    "    removed_ids = set()\n",
    "    print(\"Only one scrape date - no comparison possible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New URLs to Scrape\n",
    "\n",
    "URLs that appeared in the current scrape but weren't in the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New URLs to scrape: 13\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>portal</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>url</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>idealista</td>\n",
       "      <td>ideal_34602221</td>\n",
       "      <td>https://www.idealista.it/immobile/34602221/</td>\n",
       "      <td>470000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>idealista</td>\n",
       "      <td>ideal_34638590</td>\n",
       "      <td>https://www.idealista.it/immobile/34638590/</td>\n",
       "      <td>610000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>idealista</td>\n",
       "      <td>ideal_34635714</td>\n",
       "      <td>https://www.idealista.it/immobile/34635714/</td>\n",
       "      <td>550000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>immobiliare</td>\n",
       "      <td>immo_126177541</td>\n",
       "      <td>https://www.immobiliare.it/annunci/126177541/</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>immobiliare</td>\n",
       "      <td>immo_126018123</td>\n",
       "      <td>https://www.immobiliare.it/annunci/126018123/</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>immobiliare</td>\n",
       "      <td>immo_126170915</td>\n",
       "      <td>https://www.immobiliare.it/annunci/126170915/</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>immobiliare</td>\n",
       "      <td>immo_126171269</td>\n",
       "      <td>https://www.immobiliare.it/annunci/126171269/</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>immobiliare</td>\n",
       "      <td>immo_126108861</td>\n",
       "      <td>https://www.immobiliare.it/annunci/126108861/</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>immobiliare</td>\n",
       "      <td>immo_124590193</td>\n",
       "      <td>https://www.immobiliare.it/annunci/124590193/</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>immobiliare</td>\n",
       "      <td>immo_126011745</td>\n",
       "      <td>https://www.immobiliare.it/annunci/126011745/</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>immobiliare</td>\n",
       "      <td>immo_126093937</td>\n",
       "      <td>https://www.immobiliare.it/annunci/126093937/</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>immobiliare</td>\n",
       "      <td>immo_126125431</td>\n",
       "      <td>https://www.immobiliare.it/annunci/126125431/</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>immobiliare</td>\n",
       "      <td>immo_126080821</td>\n",
       "      <td>https://www.immobiliare.it/annunci/126080821/</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         portal      listing_id  \\\n",
       "0     idealista  ideal_34602221   \n",
       "1     idealista  ideal_34638590   \n",
       "2     idealista  ideal_34635714   \n",
       "3   immobiliare  immo_126177541   \n",
       "4   immobiliare  immo_126018123   \n",
       "5   immobiliare  immo_126170915   \n",
       "6   immobiliare  immo_126171269   \n",
       "7   immobiliare  immo_126108861   \n",
       "8   immobiliare  immo_124590193   \n",
       "9   immobiliare  immo_126011745   \n",
       "10  immobiliare  immo_126093937   \n",
       "11  immobiliare  immo_126125431   \n",
       "12  immobiliare  immo_126080821   \n",
       "\n",
       "                                              url     price  \n",
       "0     https://www.idealista.it/immobile/34602221/  470000.0  \n",
       "1     https://www.idealista.it/immobile/34638590/  610000.0  \n",
       "2     https://www.idealista.it/immobile/34635714/  550000.0  \n",
       "3   https://www.immobiliare.it/annunci/126177541/       NaN  \n",
       "4   https://www.immobiliare.it/annunci/126018123/       NaN  \n",
       "5   https://www.immobiliare.it/annunci/126170915/       NaN  \n",
       "6   https://www.immobiliare.it/annunci/126171269/       NaN  \n",
       "7   https://www.immobiliare.it/annunci/126108861/       NaN  \n",
       "8   https://www.immobiliare.it/annunci/124590193/       NaN  \n",
       "9   https://www.immobiliare.it/annunci/126011745/       NaN  \n",
       "10  https://www.immobiliare.it/annunci/126093937/       NaN  \n",
       "11  https://www.immobiliare.it/annunci/126125431/       NaN  \n",
       "12  https://www.immobiliare.it/annunci/126080821/       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show new URLs to scrape\n",
    "if not df_new_urls.empty:\n",
    "    print(f\"New URLs to scrape: {len(df_new_urls)}\\n\")\n",
    "    display(df_new_urls[['portal', 'listing_id', 'url', 'price']].reset_index(drop=True))\n",
    "else:\n",
    "    print(\"No new URLs to scrape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Parse Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026_01_18: 152 listings parsed\n",
      "2026_01_24: 13 listings parsed\n",
      "\n",
      "Total: 165 listings parsed\n"
     ]
    }
   ],
   "source": [
    "# Parse listings from all scraped dates\n",
    "all_listings = []\n",
    "\n",
    "for date_str in ALL_DATES:\n",
    "    listings_dir = SCRAPED_DIR / date_str\n",
    "    df = process_listings_directory(listings_dir, date_str)\n",
    "    \n",
    "    if not df.empty:\n",
    "        all_listings.append(df)\n",
    "        print(f\"{date_str}: {len(df)} listings parsed\")\n",
    "    else:\n",
    "        print(f\"{date_str}: No listing HTML files found\")\n",
    "\n",
    "# Combine all listings\n",
    "if all_listings:\n",
    "    df_listings = pd.concat(all_listings, ignore_index=True)\n",
    "    print(f\"\\nTotal: {len(df_listings)} listings parsed\")\n",
    "else:\n",
    "    df_listings = pd.DataFrame()\n",
    "    print(\"\\nNo listings parsed yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Final DataFrame\n",
    "\n",
    "Add `is_sold`, `prices` (history from search results), and `days_live`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataframe: 165 listings\n",
      "  Sold: 12\n",
      "  Active: 153\n",
      "  Days live (sold): min=11, max=119, avg=80\n"
     ]
    }
   ],
   "source": [
    "# Build final dataframe\n",
    "if not df_listings.empty:\n",
    "    df = df_listings.copy()\n",
    "    \n",
    "    # Get the most recent scrape date (current)\n",
    "    current_date = sorted_dates[-1] if sorted_dates else None\n",
    "    \n",
    "    # is_sold: True if listing was seen before but not in the latest search results\n",
    "    def check_sold(lid):\n",
    "        last_seen = last_seen_date.get(lid)\n",
    "        if last_seen and current_date:\n",
    "            return last_seen != current_date\n",
    "        return False\n",
    "    \n",
    "    df['is_sold'] = df['listing_id'].apply(check_sold)\n",
    "    \n",
    "    # prices: list of prices, only adding when price changed\n",
    "    def get_prices(lid):\n",
    "        history = price_history.get(lid, [])\n",
    "        prices = []\n",
    "        for (d, p) in history:\n",
    "            if p is not None:\n",
    "                # Only add if different from last price\n",
    "                if not prices or p != prices[-1]:\n",
    "                    prices.append(p)\n",
    "        return prices if prices else None\n",
    "    \n",
    "    df['prices'] = df['listing_id'].apply(get_prices)\n",
    "    \n",
    "    # days_live: for sold listings, calculate days from created_at to removal\n",
    "    def calc_days_live(row):\n",
    "        if not row['is_sold']:\n",
    "            return None\n",
    "        \n",
    "        # Get listing date from created_at (Unix timestamp for Immobiliare)\n",
    "        listing_date = None\n",
    "        if 'created_at' in row.index and pd.notna(row.get('created_at')):\n",
    "            val = row['created_at']\n",
    "            # Check if it's a Unix timestamp (large number)\n",
    "            if isinstance(val, (int, float)) and val > 1e9:\n",
    "                listing_date = pd.to_datetime(val, unit='s')\n",
    "            else:\n",
    "                listing_date = pd.to_datetime(val, errors='coerce')\n",
    "        \n",
    "        if listing_date is None or pd.isna(listing_date):\n",
    "            return None\n",
    "        \n",
    "        # Get removal date (the date after last_seen)\n",
    "        last_seen = last_seen_date.get(row['listing_id'])\n",
    "        if last_seen:\n",
    "            try:\n",
    "                idx = sorted_dates.index(last_seen)\n",
    "                if idx + 1 < len(sorted_dates):\n",
    "                    removal_date = pd.to_datetime(sorted_dates[idx + 1], format='%Y_%m_%d')\n",
    "                    return (removal_date - listing_date).days\n",
    "            except:\n",
    "                pass\n",
    "        return None\n",
    "    \n",
    "    df['days_live'] = df.apply(calc_days_live, axis=1)\n",
    "    \n",
    "    print(f\"Final dataframe: {len(df)} listings\")\n",
    "    print(f\"  Sold: {df['is_sold'].sum()}\")\n",
    "    print(f\"  Active: {(~df['is_sold']).sum()}\")\n",
    "    \n",
    "    # Show days_live stats for sold\n",
    "    sold_days = df[df['is_sold']]['days_live'].dropna()\n",
    "    if len(sold_days) > 0:\n",
    "        print(f\"  Days live (sold): min={sold_days.min():.0f}, max={sold_days.max():.0f}, avg={sold_days.mean():.0f}\")\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "    print(\"No listings to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "**Final DataFrame (`df`) columns:**\n",
    "- `listing_id`, `portal`, `url`, `title`, `price`, etc. (from parsed HTML)\n",
    "- `is_sold` - True if listing disappeared from search results\n",
    "- `prices` - List of prices (only when changed)\n",
    "- `days_live` - Days from `created_at` to removal date (for sold listings)\n",
    "- `created_at` - Listing date (Unix timestamp, both portals)\n",
    "\n",
    "**Other variables:**\n",
    "- `df_new_urls` - New URLs to scrape (from latest comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Deduplicate Listings\n",
    "\n",
    "Remove duplicate listings that appear on both Immobiliare and Idealista.\n",
    "\n",
    "**Matching criteria:**\n",
    "- Exact same price\n",
    "- Exact same surface area\n",
    "- Coordinates within 100m\n",
    "\n",
    "Keeps the Immobiliare listing and merges any additional data from the Idealista duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before deduplication: 165 listings\n",
      "  Immobiliare: 97\n",
      "  Idealista: 68\n",
      "Found 22 duplicate listings\n",
      "  Merging: ideal_24217142 -> immo_125271473\n",
      "  Merging: ideal_24851423 -> immo_125233655\n",
      "  Merging: ideal_25075109 -> immo_125522717\n",
      "  Merging: ideal_31845035 -> immo_117757313\n",
      "  Merging: ideal_32148437 -> immo_118939011\n",
      "  Merging: ideal_32198336 -> immo_119038273\n",
      "  Merging: ideal_32551850 -> immo_125402535\n",
      "  Merging: ideal_32693488 -> immo_125699521\n",
      "  Merging: ideal_33075416 -> immo_121549264\n",
      "  Merging: ideal_33121115 -> immo_121674072\n",
      "  Merging: ideal_33404330 -> immo_122526106\n",
      "  Merging: ideal_33569998 -> immo_125930031\n",
      "  Merging: ideal_33724493 -> immo_123422763\n",
      "  Merging: ideal_33909723 -> immo_123937121\n",
      "  Merging: ideal_33934638 -> immo_124014459\n",
      "  Merging: ideal_33993703 -> immo_124174799\n",
      "  Merging: ideal_34180958 -> immo_124722345\n",
      "  Merging: ideal_34283360 -> immo_125070539\n",
      "  Merging: ideal_34297868 -> immo_125106347\n",
      "  Merging: ideal_34305642 -> immo_125131153\n",
      "  Merging: ideal_34362106 -> immo_124268125\n",
      "  Merging: ideal_34519212 -> immo_125374999\n",
      "Removed 22 duplicate Idealista listings\n",
      "\n",
      "After deduplication: 143 listings\n",
      "  Immobiliare: 97\n",
      "  Idealista: 46\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate listings across portals\n",
    "# Keeps Immobiliare listings and merges data from duplicate Idealista listings\n",
    "\n",
    "if not df.empty:\n",
    "    print(f\"Before deduplication: {len(df)} listings\")\n",
    "    print(f\"  Immobiliare: {len(df[df.portal == 'immobiliare'])}\")\n",
    "    print(f\"  Idealista: {len(df[df.portal == 'idealista'])}\")\n",
    "    \n",
    "    df = deduplicate_listings(df)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Fix list columns for parquet serialization\n",
    "    df = fix_list_columns(df)\n",
    "    \n",
    "    print(f\"\\nAfter deduplication: {len(df)} listings\")\n",
    "    print(f\"  Immobiliare: {len(df[df.portal == 'immobiliare'])}\")\n",
    "    print(f\"  Idealista: {len(df[df.portal == 'idealista'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 143 cached derived_fields\n",
      "Matched 143/143 listings from cache\n"
     ]
    }
   ],
   "source": [
    "# Load existing derived_fields from JSON cache\n",
    "# This is separate from the parquet and persists between runs\n",
    "\n",
    "import json\n",
    "\n",
    "derived_fields_path = Path('data/processed/derived_fields.json')\n",
    "\n",
    "if derived_fields_path.exists():\n",
    "    with open(derived_fields_path) as f:\n",
    "        derived_cache = json.load(f)\n",
    "    print(f\"Loaded {len(derived_cache)} cached derived_fields\")\n",
    "else:\n",
    "    derived_cache = {}\n",
    "    print(\"No derived_fields cache found, starting fresh\")\n",
    "\n",
    "# Add derived_fields column to df from cache\n",
    "if not df.empty:\n",
    "    df['derived_fields'] = df['id'].apply(lambda x: derived_cache.get(x))\n",
    "    cached = df['derived_fields'].notna().sum()\n",
    "    print(f\"Matched {cached}/{len(df)} listings from cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listings missing derived_fields: 0/143\n",
      "Filled 1235 missing values from derived fields\n",
      "Final columns: 73\n"
     ]
    }
   ],
   "source": [
    "# Derive missing fields using Gemini (runs in parallel with 10 workers)\n",
    "# Only processes listings not in the cache\n",
    "\n",
    "if not df.empty:\n",
    "    missing = df['derived_fields'].isna().sum()\n",
    "    print(f\"Listings missing derived_fields: {missing}/{len(df)}\")\n",
    "    \n",
    "    if missing > 0:\n",
    "        df = derive_fields_for_dataset(df, DATA_DIR, max_workers=10)\n",
    "        \n",
    "        # Update cache with all successful derived_fields (new or retried)\n",
    "        updated = 0\n",
    "        for _, row in df.iterrows():\n",
    "            if row['derived_fields']:  # If we have a valid result\n",
    "                if row['id'] not in derived_cache:\n",
    "                    derived_cache[row['id']] = row['derived_fields']\n",
    "                    updated += 1\n",
    "        \n",
    "        # Save updated cache\n",
    "        with open(derived_fields_path, 'w') as f:\n",
    "            json.dump(derived_cache, f, indent=2)\n",
    "        print(f\"Added {updated} new derived_fields to cache (total: {len(derived_cache)})\")\n",
    "    \n",
    "    # Reload cache to get all derived fields\n",
    "    with open(derived_fields_path) as f:\n",
    "        derived_cache = json.load(f)\n",
    "    \n",
    "    # Mapping from derived field names to dataframe column names\n",
    "    # (derived_key, df_column, is_new_column)\n",
    "    field_mapping = [\n",
    "        ('summary', 'ai_summary', True),  # New column\n",
    "        ('beauty_score', 'beauty_score', True),  # New column (1-5 rating)\n",
    "        ('beauty_notes', 'beauty_notes', True),  # New column (explanation)\n",
    "        ('bedrooms', 'bedrooms', False),  # Exists\n",
    "        ('bathrooms', 'bathrooms', False),  # Exists\n",
    "        ('kitchen_type', 'kitchen', False),  # Exists as 'kitchen'\n",
    "        ('balconies', 'balconies', True),  # New column (count)\n",
    "        ('terraces', 'terraces', True),  # New column (count)\n",
    "        ('has_cantina', 'has_cellar', False),  # Exists as 'has_cellar'\n",
    "        ('has_garage', 'has_garage', True),  # New column\n",
    "        ('parking_spots', 'parking_spots', True),  # New column\n",
    "        ('has_elevator', 'elevator', False),  # Exists as 'elevator'\n",
    "        ('floor_number', 'floor', False),  # Exists as 'floor'\n",
    "        ('total_floors', 'floors_building', False),  # Exists\n",
    "        ('has_air_conditioning', 'has_air_conditioning', False),  # Exists\n",
    "        ('heating_type', 'heating', False),  # Exists\n",
    "        ('condition', 'condition', False),  # Exists\n",
    "        ('exposure', 'exposure', True),  # New column\n",
    "        ('has_garden', 'has_garden', False),  # Exists\n",
    "        ('energy_class', 'energy_class', False),  # Exists\n",
    "    ]\n",
    "    \n",
    "    # Add new columns if they don't exist\n",
    "    for derived_key, df_col, is_new in field_mapping:\n",
    "        if is_new and df_col not in df.columns:\n",
    "            df[df_col] = None\n",
    "    \n",
    "    # Fill missing values from derived fields\n",
    "    filled_count = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        derived = derived_cache.get(row['id'], {})\n",
    "        if not derived:\n",
    "            continue\n",
    "        \n",
    "        for derived_key, df_col, _ in field_mapping:\n",
    "            if derived_key not in derived:\n",
    "                continue\n",
    "            derived_val = derived[derived_key]\n",
    "            \n",
    "            # Skip invalid values\n",
    "            if derived_val is None or derived_val == -1 or derived_val == '':\n",
    "                continue\n",
    "            \n",
    "            # Check if current value is missing\n",
    "            current_val = row.get(df_col)\n",
    "            is_missing = current_val is None or (isinstance(current_val, float) and pd.isna(current_val)) or current_val == ''\n",
    "            \n",
    "            if is_missing:\n",
    "                # Convert value to match column dtype if needed\n",
    "                col_dtype = df[df_col].dtype\n",
    "                if pd.api.types.is_string_dtype(col_dtype) and not isinstance(derived_val, str):\n",
    "                    derived_val = str(derived_val)\n",
    "                \n",
    "                df.at[idx, df_col] = derived_val\n",
    "                filled_count += 1\n",
    "    \n",
    "    print(f\"Filled {filled_count} missing values from derived fields\")\n",
    "    \n",
    "    # Drop the derived_fields column - we've merged into flat columns\n",
    "    if 'derived_fields' in df.columns:\n",
    "        df = df.drop(columns=['derived_fields'])\n",
    "    \n",
    "    print(f\"Final columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/processed/listings.parquet\n",
      "Saved: data/processed/new_urls_to_scrape.txt (13 URLs)\n"
     ]
    }
   ],
   "source": [
    "# Save final dataframe\n",
    "output_dir = Path('data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not df.empty:\n",
    "    df.to_parquet(output_dir / 'listings.parquet', index=False)\n",
    "    print(f\"Saved: {output_dir / 'listings.parquet'}\")\n",
    "    \n",
    "# Export new URLs to scrape\n",
    "if not df_new_urls.empty:\n",
    "    urls_file = output_dir / 'new_urls_to_scrape.txt'\n",
    "    with open(urls_file, 'w') as f:\n",
    "        for url in df_new_urls['url']:\n",
    "            f.write(url + '\\n')\n",
    "    print(f\"Saved: {urls_file} ({len(df_new_urls)} URLs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
