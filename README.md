# ğŸ  HouseHunter Pro: Local Real Estate Archive

**Project Type:** Python Data Pipeline (Pandas + Parquet + Jupyter)
**Objective:** Scrape and analyze real estate listings from Idealista/Immobiliare using parquet files, pandas dataframes, and Jupyter notebooks.

---

## ğŸ— Architecture

### Jupyter-First Workflow
This project uses **Jupyter notebooks** for all data processing and analysis. Raw scraping is done via command-line tools, but everything else happens in notebooks.

### The Data Flow

```
1. SCRAPE â†’ Raw HTML + Images (command line)
2. PROCESS â†’ Jupyter notebook imports functions and creates dataframes
3. ANALYZE â†’ All analysis in Jupyter with pandas
4. EXPORT â†’ Save to parquet, Excel, or CSV
```

---

## ğŸ“ Project Structure

```
/houses
â”œâ”€â”€ househunter_workflow.ipynb     # Main Jupyter notebook (your workflow)
â”œâ”€â”€ scraping/                      # Python modules (imported by notebook)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ process_listings_to_parquet.py
â”‚   â”œâ”€â”€ process_search_results_to_parquet.py
â”‚   â”œâ”€â”€ compare_search_results.py
â”‚   â”œâ”€â”€ extract_immobiliare.py
â”‚   â””â”€â”€ extract_idealista.py
â”œâ”€â”€ src/backend/                   # Scraping scripts
â”‚   â”œâ”€â”€ manual_scraper.py          # Listing scraper
â”‚   â””â”€â”€ manual_scraper_search.py   # Search results scraper
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ scraped/                   # Raw HTML + images
â”‚   â”‚   â””â”€â”€ YYYY_MM_DD/            # Daily snapshots
â”‚   â”œâ”€â”€ listings/                  # Processed parquet files
â”‚   â”œâ”€â”€ search_results/            # Processed search results
â”‚   â””â”€â”€ comparisons/               # Comparison outputs
â”œâ”€â”€ Makefile                       # Helper commands
â””â”€â”€ README.md                      # This file
```

---

## ğŸš€ Quick Start

### 1. Installation

```bash
make install
```

This installs all dependencies and Playwright browsers.

### 2. Scraping (Week 1)

```bash
# Scrape search results
make scrape_search_results
# Navigate through all pages in Chrome, press ENTER for each page

# Scrape individual listings (optional for week 1)
make scrape
# Navigate to listings, press ENTER for each
```

**Output:** `data/scraped/2026_01_18/` (or whatever today's date is)

### 3. Processing & Analysis

```bash
# Open Jupyter
make jupyter

# Then open: househunter_workflow.ipynb
# Follow the notebook cells to process and analyze data
```

The notebook will:
- Process search results â†’ dataframe â†’ parquet
- Process listings â†’ dataframe â†’ parquet
- Compare snapshots to find new listings
- Analyze and visualize the data
- Export to Excel

### 4. Next Week (Week 2)

```bash
# Scrape this week's search results
make scrape_search_results

# Open Jupyter
make jupyter

# In the notebook:
# - Update CURRENT_DATE to this week
# - Run cells to process new search results
# - Compare with last week to find NEW listings
# - The comparison outputs URLs to scrape

# Scrape only the NEW listings
make scrape
# Use the URLs from comparison output

# Process new listings in notebook
```

---

## ğŸ“Š The Jupyter Notebook Workflow

Open `househunter_workflow.ipynb` and follow these sections:

### Section 1: Process Search Results
```python
# Imports functions from scraping/ module
from scraping import process_search_results_directory

# Processes HTML â†’ pandas dataframe â†’ parquet
df_search = process_search_results_directory(...)
```

### Section 2: Process Listings
```python
from scraping import process_date_directory

# Processes listing HTML â†’ pandas dataframe â†’ parquet
df_listings = process_date_directory(...)
```

### Section 3: Compare Snapshots
```python
from scraping import compare_snapshots

# Compares two weeks, finds new listings
comparison = compare_snapshots(current, previous)
new_listings = comparison['new']  # URLs to scrape
```

### Section 4: Analyze Data
```python
# All standard pandas operations
df = pd.read_parquet('data/listings/2026_01_24.parquet')

# Filter
best_deals = df.nsmallest(10, 'price_per_sqm')

# Visualize
df['price'].hist()

# Export
df.to_excel('listings.xlsx')
```

---

## ğŸ¯ Portal Targets

### Immobiliare.it
```
https://www.immobiliare.it/vendita-case/bologna/centro/
  ?prezzoMinimo=450000&prezzoMassimo=700000
  &superficieMinima=120&localiMinimo=4
```

### Idealista.it
```
https://www.idealista.it/vendita-case/bologna/centro/
  con-prezzo_700000,prezzo-min_450000,
  dimensione_120,quadrilocali-4
```

---

## ğŸ›  Available Commands

```bash
make install               # Install all dependencies
make scrape                # Scrape individual listings
make scrape_search_results # Scrape search result pages
make jupyter               # Open Jupyter notebook
```

**All processing happens in the Jupyter notebook!**

---

## ğŸ“¦ Dependencies

**Core:**
- `pandas` - Data manipulation
- `pyarrow` - Parquet file support
- `jupyter` - Notebook interface
- `playwright` - Browser automation
- `beautifulsoup4` - HTML parsing

**Optional:**
- `openpyxl` - Excel export
- `matplotlib` - Visualizations

---

## ğŸ’¡ Key Benefits

### Why Jupyter?
- **Interactive**: See results immediately, iterate quickly
- **Reproducible**: Re-run cells as needed
- **Visual**: Built-in plotting and dataframe display
- **Flexible**: Import functions from `scraping/` module
- **Shareable**: Export notebooks or Excel files

### Why Parquet?
- **Fast**: Columnar format, optimized for analytics
- **Small**: ~70% compression vs JSON
- **Typed**: Preserves data types (dates, numbers, booleans)
- **Portable**: Works with pandas, DuckDB, Spark, etc.

### Why Daily Snapshots?
- **Track changes**: Compare this week vs last week
- **Price history**: See price drops over time
- **Market trends**: Analyze listing velocity
- **Incremental**: Only scrape new listings

---

## ğŸ“ Data Schema

### Search Results Parquet
```python
{
    'listing_id': 'immo_123456',
    'portal': 'immobiliare',
    'url': 'https://...',
    'page_number': 1,
    'position': 5,
    'search_url': 'https://...',
    'snapshot_date': '2026-01-24'
}
```

### Listings Parquet
```python
{
    'listing_id': 'immo_123456',
    'portal': 'immobiliare',
    'url': 'https://...',
    'title': 'Apartment in Centro',
    'price': 550000,
    'surface_sqm': '120 m2',
    'rooms': '4 locali',
    'bathrooms': 2,
    'floor': '2',
    'latitude': 44.4949,
    'longitude': 11.3426,
    'price_per_sqm': 4583,
    'energy_class': 'E',
    'has_elevator': True,
    'photo_count': 15,
    'snapshot_date': '2026-01-24'
}
```

---

## ğŸ” Example Analyses

### Find Best Deals
```python
df = pd.read_parquet('data/listings/2026_01_24.parquet')
df.nsmallest(10, 'price_per_sqm')
```

### Compare Prices
```python
df_old = pd.read_parquet('data/listings/2026_01_18.parquet')
df_new = pd.read_parquet('data/listings/2026_01_24.parquet')

merged = df_old.merge(df_new, on='listing_id', suffixes=('_old', '_new'))
price_drops = merged[merged['price_new'] < merged['price_old']]
```

### Custom Filters
```python
filtered = df[
    (df['price'] >= 450000) &
    (df['price'] <= 650000) &
    (df['surface_numeric'] >= 120)
]
```

### Export to Excel
```python
df.to_excel('listings_2026_01_24.xlsx', index=False)
```

---

## ğŸ“ Notes

- Scrapers save to `data/scraped/` instead of `src/backend/storage/`
- All processing happens in Jupyter notebook
- Import functions from `scraping/` module
- Parquet files are gitignored
- Each week is a separate snapshot

---

## ğŸ“ Tips

1. **Weekly workflow**: Scrape search results â†’ compare â†’ scrape new listings
2. **Version dates**: Use `YYYY_MM_DD` format consistently
3. **Notebook workflow**: Process â†’ analyze â†’ export, all in one place
4. **Reusable functions**: Import from `scraping/` module
5. **Interactive analysis**: Iterate quickly in Jupyter

---

## ğŸ†˜ Troubleshooting

**Can't import scraping module?**
```python
import sys
sys.path.insert(0, '/Users/lspataro/projects/houses')
from scraping import process_date_directory
```

**Chrome not starting?**
```bash
# Close all Chrome windows first
pkill -9 Chrome
make scrape
```

**Module not found?**
```bash
make install  # Reinstall dependencies
```

---

For quick reference, see [QUICK_START.md](QUICK_START.md)
